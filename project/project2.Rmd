---
title: "Project 2"
author: "SDS348 Fall 2020"
date: "December 2, 2020"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
---

```{r global_options, include=FALSE}
#DO NOT EDIT THIS CHUNK OR ANYTHING ABOVE IT!
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F, tidy=T, tidy.opts=list(width.cutoff=60),R.options=list(max.print=100))
```
## Braeden Conrad (bsc875)

## Credit Card Application Analysis

### Introduction
This data set, called `CreditCard`, contains cross-section data on the credit history for a sample of applicants for a type of credit card.

It has 1,319 observations and 12 variables, which are:

  - card - whether or not the application was accepted
  - reports - number of derogatory reports
  - age - age in years plus twelfths of a year
  - income - in USD (10,000)
  - share - ratio of card expenditure to yearly income
  - owner - whether or not the individual owns a home
  - selfemp - self employed or not?
  - dependents - number of dependents
  - months - months living at current address
  - majorcards - number of major credit cards held
  - active - number of active credit accounts

```{r}
library(tidyverse)
install.packages("AER",repos = "http://cran.us.r-project.org")
library(AER)
data("CreditCard")
CreditCard <- na.omit(CreditCard)
CreditCard$card <- ifelse(CreditCard$card=="yes",1,0)
```

### MANOVA

#### Looking at if there is a mean difference in homeowners vs. non-homeowners
```{r}
man <- manova(cbind(reports,income,age,share,expenditure,dependents,months,majorcards,active)~owner,data=CreditCard)
summary(man)
```
There appears to be a mean difference between the groups across numeric variables.

```{r}
summary.aov(man)
```
Every numeric variable was significant in the difference between home owners and non home owners except for `share` and `reports`. 

Post-hoc t tests do not need to be done because there are only two groups. 

In total, there was 1 MANOVA test and 9 ANOVA tests. The probability of a Type-1 error is $1-.95^{10}=0.401$. The Bonaferri correction adjusts the siginificance level from 0.05 to $\frac{.05}{10}=0.005$. This means that `majorcards` is not significantly different between home owners and non home owners, along with `reports` and `share`. 

#### Testing MANOVA assumptions
```{r}
library(rstatix)

group <- CreditCard$owner 
DVs <- CreditCard %>% select(reports,income,age,share,expenditure,dependents,months,majorcards,active)

#Test multivariate normality for each group (null: assumption met)
sapply(split(DVs,group), mshapiro_test)

#If any p<.05, stop (assumption violated). If not, test homogeneity of covariance matrices

#Box's M test (null: assumption met)
box_m(DVs, group)

#Optionally, view covariance matrices for each group
lapply(split(DVs,group), cov)
```
The multivariate normality assumption is met. Box's M test also demonstrates homogeneity of covariance matrices exists.

### Randomization Tests

#### Mean Difference in Income for those that are self employed?
```{r}
set.seed(1234)

# get original difference
orig_diff <- abs(mean(CreditCard[CreditCard$selfemp=="yes",]$income)-mean(CreditCard[CreditCard$selfemp=="no",]$income))
# randomization test
diff <- vector()
n <- 5000
for (i in 1:n){
  new <- data.frame(income=sample(CreditCard$income),selfemp=CreditCard$selfemp)
  diff[i] <- mean(new[new$selfemp=="yes",]$income)-mean(new[new$selfemp=="no",]$income)
}

hist(diff)
abline(v=c(orig_diff,-orig_diff),col="red")
mean(diff>orig_diff | diff < (-orig_diff))
```

This shows that being self employed or not significantly predicts a mean difference in income between the two groups. 

### Linear Regression

#### Trying to predict age from other variables
```{r}
cc <- CreditCard%>%
  mutate(income_c=income-mean(income))
fit <- lm(age~income_c+months+selfemp+selfemp:income_c,data=cc)
summary(fit)
```
The intercept coefficient, 29.71, represents the average age of someone applying for credit card who is not self employed, has spent 0 months at a new address, and has an average income. 

The coefficient for income_c, 1.74, represents a 1.74 year age increase for each unit of income ($10,000) above the average income, for those who are not self employed and months are held equal. 

The coefficient for self employment, 2.57, represents a +2.57 year average age difference between those who are self employed and those who aren't.

The coefficient for the interaction between income and self-employment, -1.307, indicates a $1.74-1.31=0.43$ year age increase for each unit of income above the average, for those who are self employed and months are held equal. 

The coefficient for months, 0.0616, means that every month at the same address represents a 0.0616 year age increase. 

```{r}
ggplot(cc,aes(x=months,y=age))+
  geom_point()+
  geom_smooth(method="lm")
ggplot(cc,aes(x=income_c,y=age,col=selfemp))+
  geom_point()+
  geom_smooth(method="lm")
```
```{r}
# check for homeskedasticity
library(sandwich)
library(lmtest)
bptest(fit)
```
The Breusch-Pagan test indicates this model is heteroskedastic. Must use robust standard errors:
```{r}
coeftest(fit,vcov=vcovHC(fit))
```
All of the variables are still significant, and the coefficients are the same. The standard error is larger for the self employment variable than the original.

The $R^2$ statistic found earlier was 0.2698, which indicates that about 27% of variation of the age is explained by the model, which is poor. 

#### Bootstrapped SE
```{r}
coef_vals <- matrix(nrow = 1000,ncol=5)
for (i in 1:1000){
  data <- cc[sample(nrow(cc),replace=T),]
  fit1 <- lm(age~income_c+months+selfemp+selfemp:income_c,data=data)
  coef_vals[i,] <- coef(fit1)
}
se <- apply(coef_vals,2,sd)
means <- apply(coef_vals,2,mean)
x <- data.frame(SE=se,orig_SE=summary(fit)$coefficients[, 2],coef=means,orig_coef=coef(fit))
x
```
The bootstrap standard error is  very similar to the robust standard errors, and the coefficients are similar too. 

### Logistic Regression
```{r}
class_diag<-function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  f1=2*(sens*ppv)/(sens+ppv)

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE){
    truth<-as.numeric(truth)-1}
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,f1,auc)
}
```

#### Predicting credit card application acceptance
```{r}
CreditCard <- mutate(CreditCard,income_c=income-mean(income))
glm_fit <- glm(card~reports+income_c,data=CreditCard,family="binomial")
summary(glm_fit)
exp(coef(glm_fit))
```
The coefficient for the intercept, 1.91, is the logodds of having the credit card accepted with an average income and 0 reports. 

The coefficient for the reports, -1.353, is the logodds change for each derogatory report a person has. The exponentiated coefficient $e^{-1.35}=0.258$ shows that it reduces the odds of having the credit card accepted. 

The coefficient for centered income, 0.256, is the logodds change for each unit increase in income. The exponentiated coefficient, 1.292, shows that the odds increase by 1.29 for each unit increase in income above the average.
```{r}
table(pred=glm_fit$fitted.values>0.5,true=as.logical(CreditCard$card))%>%addmargins
```
```{r}
class_diag(glm_fit$fitted.values,CreditCard$card)
```
The AUC is 0.77, which is a fair model. The biggest issue the model has is with detecting the negatives, which are the denials for getting a credit card. This indicates that it would help for the threshold to be lowered from probability of 0.5. 
```{r}
logit <- predict(glm_fit,CreditCard,type="link")
data <- CreditCard %>% mutate(logit=logit,card=as.factor(card))
ggplot(data)+
  geom_point(aes(x=income,y=logit,col=card))
```
```{r}
install.packages("ROSE",repos = "http://cran.us.r-project.org")
library(ROSE)
roc.curve(CreditCard$card,glm_fit$fitted.values)
```
The area under the curve is 0.77 which is fair, and the curve has a fairly normal shape. 

#### Using all predictors
```{r}
CreditCard <- select(CreditCard,-income_c)
glm_fit2 <- glm(card~.,data=CreditCard,family="binomial")
class_diag(glm_fit2$fitted.values,CreditCard$card)
```
The AUC actually decreased using all of the predictors. The issue again was the true negative rate, because it did not identify when the credit card was not given. 

Now, using 10-fold CV:
```{r}
set.seed(5432)
k <-10
data <- CreditCard[sample(nrow(CreditCard)),]
folds <- cut(seq(1:nrow(CreditCard)),breaks=k,labels=F)
diags <- NULL
for (i in 1:k){
  train <- data[folds!=i,]
  test <- data[folds==i,]
  
  fit <- glm(card~.,data=train,family="binomial")
  probs <- predict(fit,newdata=test,type="response")
  diags <-rbind(diags,class_diag(probs,test$card))
}
apply(diags,2,mean)
```
By doing the 10-fold CV, the specificity increased to 0.532, which was higher than the previous model, and also showed a larger AUC of 0.75, which is closer to the previous model which was 0.77.

Now trying with Lasso:
```{r}
library(glmnet)
set.seed(1234)

card_X <- model.matrix(~.,CreditCard%>%select(-card))
card_y <- as.matrix(CreditCard$card)
cv.lasso1 <- cv.glmnet(x=card_X,y=card_y,family="binomial")
lasso_fit<-glmnet(x=card_X,y=card_y,family="binomial",alpha=1,lambda=cv.lasso1$lambda.1se)
coef(lasso_fit)
```
The variables that were retained from the lasso fit were all of them. Therefore it isn't necessary to do another 10-fold CV on all of the same variables. 


```{R, echo=F}
## DO NOT DELETE OR MODIFY THIS CHUNK: IT MUST BE PRESENT TO RECEIVE CREDIT FOR THE ASSIGNMENT
sessionInfo(); Sys.time(); Sys.info()
```